{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Part Type Collision Analysis\n",
    "\n",
    "This notebook will contain gathered results from experiment 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Type Collision Analysis\n",
    "## Methodology\n",
    "For each part type we have, run the experiment many times over many different hyperparameters. Specifically, isolate one hyperparameter, run the experiment over a range of values, tracking the computed collision rate each time. Repeat this for each hyperparameter and each part type.\n",
    "## Deliverables\n",
    "Graphs and analysis for the impact of different values of the hyperparmeters. How do they affect the final collision rate? Why are the effecting the collision rate like that? What does this tell us? \n",
    "Graphs and analysis for comparing the results across different part types. Are different part types affected in the same way by the same change in hyperperamters? How close are their collision rates? What does this tell us about the relative importance of both hyperparameters and part types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Environment\n",
    "Installing required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing Directory to Parent to allow importing external data files. Please change the specified path based on where this repo exists for you locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "user_path = '~/GitHub/matcher'  # CHANGE THIS LINE AS NEEDED FOR YOUR ENVIRONMENT\n",
    "os.chdir(os.path.expanduser(user_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code\n",
    "\n",
    "The below sections contains all of our source codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import scipy.stats as st\n",
    "from scipy import stats\n",
    "import dataclasses\n",
    "import argparse\n",
    "import sys\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.artifacts import download_artifacts\n",
    "\n",
    "@dataclass\n",
    "class NormalDistribution:\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "@dataclass\n",
    "class Part:\n",
    "\n",
    "    type: str\n",
    "    sub_part_name: str\n",
    "    sensor: str\n",
    "    signals: List   # Signal is numpy array of (500,3) with [frequency, Z, X]\n",
    "\n",
    "\n",
    "def load_part_data(part_type: str) -> List[Part]:\n",
    "\n",
    "    parts = []\n",
    "    for part_dir in os.listdir(f'psig_matcher/data/{part_type}'):\n",
    "\n",
    "        sensor = part_dir[1:]\n",
    "        measurement_files = glob.glob(f'psig_matcher/data/{part_type}/{part_dir}/*.npy')\n",
    "        measurements = [np.load(f) for f in measurement_files]\n",
    "        parts.append(Part(part_type, part_dir, sensor, measurements))\n",
    "\n",
    "    return parts\n",
    "\n",
    "def limit_deminsionality(parts: List[Part], frequeny_indexes: List[int]) -> List[Part]:\n",
    "    \"\"\"Use only a subset of the frequencies for the analysis. This effectivley transforms the\n",
    "    500 dimension multivariant distribution to a n-dimentional distribution where n is the\n",
    "    length of the frequency_indexes.\n",
    "    Further, this assumes use of the X axis\"\"\"\n",
    "\n",
    "    return [\n",
    "        dataclasses.replace(part, signals=[[signal[index][1] for index in frequeny_indexes] for signal in part.signals])\n",
    "        for part in parts]\n",
    "\n",
    "def compute_normal_ci(x: List[float], confidence: float) -> Tuple[float, float]:\n",
    "    \"\"\"Computes the confidence interval for a given confidence bound.\"\"\"\n",
    "\n",
    "    if np.mean(x) == 0: return (0, 0)\n",
    "    \n",
    "    if len(x) < 30:\n",
    "        return st.t.interval(confidence, len(x)-1, loc=np.mean(x), scale=st.sem(x))\n",
    "    else:\n",
    "        return stats.norm.interval(confidence, loc=np.mean(x), scale=np.std(x))\n",
    "\n",
    "def estimate_normal_dist(x: List[float], confidence: float) -> NormalDistribution:\n",
    "    \"\"\"Estimate the normal distribution for the given data.\n",
    "    This is done using: https://handbook-5-1.cochrane.org/chapter_7/7_7_3_2_obtaining_standard_deviations_from_standard_errors_and.htm#:~:text=The%20standard%20deviation%20for%20each,should%20be%20replaced%20by%205.15.\n",
    "    \"\"\"\n",
    "\n",
    "    val_comp = st.t.ppf if len(x) < 30 else stats.norm.ppf\n",
    "    lower, upper = compute_normal_ci(x, confidence)\n",
    "\n",
    "    val = val_comp(confidence, len(x)-1)\n",
    "    std = np.sqrt(len(x))*(upper-lower)*val\n",
    "    return NormalDistribution(np.mean(x, axis=0), std)\n",
    "\n",
    "\n",
    "def probability_of_multivariant_point(mu: List[float], cov: List[List[float]], x: List[float]) -> float:\n",
    "\n",
    "    #https://stats.stackexchange.com/questions/331283/how-to-calculate-the-probability-of-a-data-point-belonging-to-a-multivariate-nor\n",
    "    # Double check this math\n",
    "    m_dist_x = np.dot((x-mu).transpose(),np.linalg.inv(cov))\n",
    "    m_dist_x = np.dot(m_dist_x, (x-mu))\n",
    "    return 1-stats.chi2.cdf(m_dist_x, 3)\n",
    "\n",
    "def estimate_overlap_of_set_with_sample_signals(parts: List[Part], samples: int, meta_pdf_ci: float, part_pdf_ci: float, confidence_bound: float) -> float:\n",
    "    \"\"\" I believe this is the best solution out of all them. We are directly modeling the distribution/state space that\n",
    "    the signals come from, and sampling from that. This directly correlates with the CI and is intuitive. See notion for\n",
    "    more details and defense. \"\"\"\n",
    "\n",
    "    min_confidence = 1 - confidence_bound\n",
    "    signals = [\n",
    "        signal for part in parts\n",
    "        for signal in part.signals]\n",
    "\n",
    "    part_pdfs = [estimate_normal_dist(part.signals, part_pdf_ci) for part in parts]\n",
    "    sample_pdf = estimate_normal_dist(signals, meta_pdf_ci)\n",
    "    \n",
    "    state_space_samples = np.random.multivariate_normal(sample_pdf.mean, np.diag(sample_pdf.std), samples)\n",
    "\n",
    "    # using probability_of_multivariant_point no longer directly equates to false negative rate.\n",
    "    # TODO (henry): Figure out relationship between integrated pdf range and false negative rate\n",
    "    sample_confidences = [\n",
    "        [probability_of_multivariant_point(pdf.mean, np.diag(pdf.std), sample) for pdf in part_pdfs]\n",
    "        for sample in state_space_samples]\n",
    "\n",
    "    filtered_confidences = [\n",
    "        list(filter(lambda confidence: confidence >= min_confidence, sample_confidence))\n",
    "        for sample_confidence in sample_confidences]\n",
    "\n",
    "    # We're ok with up to 1 match, but every one more than that is a conflict.\n",
    "    collisions = [max(len(confidences)-1, 0) for confidences in filtered_confidences]\n",
    "    return sum(collisions)/(samples*len(part_pdfs))\n",
    "\n",
    "def run_meta_markov_multivariant_analysis(parts: List[Part], part_dim: int, num_samples: int, meta_pdf_ci: float, part_pdf_ci: float, confidence_bound: float):\n",
    "    \"\"\" Runs the Monte Carlo Approximation of multivariant collision using the signal sample meta\n",
    "    pdf methodology. The Monte Carlo Approximation will continually be run until the confidence interval\n",
    "    converges and the average of the previous 10 runs is not smaller than the average of the previous 100 runs.\"\"\"\n",
    "\n",
    "    collisions = []\n",
    "    confidence_ranges = []\n",
    "    while True:\n",
    "\n",
    "        multivariant_parts = limit_deminsionality(parts, list(range(part_dim)))\n",
    "        collision_rate = estimate_overlap_of_set_with_sample_signals(multivariant_parts, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\n",
    "\n",
    "        collisions.append(collision_rate)\n",
    "        mlflow.log_metric(\"monte_carlo_collision_rate\", collision_rate)\n",
    "\n",
    "        lower, upper = compute_normal_ci(collisions, 0.95)\n",
    "        confidence_ranges.append(upper - lower)\n",
    "        mlflow.log_metric(\"monte_carlo_confidence_interval\", upper - lower)\n",
    "\n",
    "        # print(f\"Estimated collision rate from sample distributiion has range: {upper - lower}\")\n",
    "\n",
    "        if len(confidence_ranges) > 100 and np.mean(confidence_ranges[-10:]) >= np.mean(confidence_ranges[-100:]):\n",
    "            return upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "The below sections gives example scenarios to illustrate the working code and validate the proposed approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Base Line*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper collision rate: 0%\n",
      "Upper collision rate: 0%\n",
      "Upper collision rate: 0%\n",
      "Upper collision rate: 0%\n",
      "Upper collision rate: 0%\n",
      "Upper collision rate: 0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mwith\u001b[39;00m mlflow\u001b[39m.\u001b[39mstart_run():\n\u001b[1;32m     54\u001b[0m     mlflow\u001b[39m.\u001b[39mlog_params(params)\n\u001b[0;32m---> 55\u001b[0m     run_experiment(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m     56\u001b[0m     mlflow\u001b[39m.\u001b[39mend_run()\n",
      "Cell \u001b[0;32mIn [8], line 4\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(part_type, part_dim, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_experiment\u001b[39m(part_type: \u001b[39mstr\u001b[39m, part_dim: \u001b[39mint\u001b[39m, num_samples: \u001b[39mint\u001b[39m, meta_pdf_ci: \u001b[39mfloat\u001b[39m, part_pdf_ci: \u001b[39mfloat\u001b[39m, confidence_bound: \u001b[39mfloat\u001b[39m):\n\u001b[1;32m      3\u001b[0m     con_parts \u001b[39m=\u001b[39m load_part_data(part_type)\n\u001b[0;32m----> 4\u001b[0m     estimated_upper_collision_rate \u001b[39m=\u001b[39m run_meta_markov_multivariant_analysis(\n\u001b[1;32m      5\u001b[0m     con_parts, part_dim, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUpper collision rate: \u001b[39m\u001b[39m{\u001b[39;00mestimated_upper_collision_rate \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [3], line 121\u001b[0m, in \u001b[0;36mrun_meta_markov_multivariant_analysis\u001b[0;34m(parts, part_dim, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     multivariant_parts \u001b[39m=\u001b[39m limit_deminsionality(parts, \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(part_dim)))\n\u001b[0;32m--> 121\u001b[0m     collision_rate \u001b[39m=\u001b[39m estimate_overlap_of_set_with_sample_signals(multivariant_parts, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\n\u001b[1;32m    123\u001b[0m     collisions\u001b[39m.\u001b[39mappend(collision_rate)\n\u001b[1;32m    124\u001b[0m     mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m\"\u001b[39m\u001b[39mmonte_carlo_collision_rate\u001b[39m\u001b[39m\"\u001b[39m, collision_rate)\n",
      "Cell \u001b[0;32mIn [3], line 99\u001b[0m, in \u001b[0;36mestimate_overlap_of_set_with_sample_signals\u001b[0;34m(parts, samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\u001b[0m\n\u001b[1;32m     95\u001b[0m state_space_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mmultivariate_normal(sample_pdf\u001b[39m.\u001b[39mmean, np\u001b[39m.\u001b[39mdiag(sample_pdf\u001b[39m.\u001b[39mstd), samples)\n\u001b[1;32m     97\u001b[0m \u001b[39m# using probability_of_multivariant_point no longer directly equates to false negative rate.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m# TODO (henry): Figure out relationship between integrated pdf range and false negative rate\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m sample_confidences \u001b[39m=\u001b[39m [\n\u001b[1;32m    100\u001b[0m     [probability_of_multivariant_point(pdf\u001b[39m.\u001b[39mmean, np\u001b[39m.\u001b[39mdiag(pdf\u001b[39m.\u001b[39mstd), sample) \u001b[39mfor\u001b[39;00m pdf \u001b[39min\u001b[39;00m part_pdfs]\n\u001b[1;32m    101\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m state_space_samples]\n\u001b[1;32m    103\u001b[0m filtered_confidences \u001b[39m=\u001b[39m [\n\u001b[1;32m    104\u001b[0m     \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m confidence: confidence \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_confidence, sample_confidence))\n\u001b[1;32m    105\u001b[0m     \u001b[39mfor\u001b[39;00m sample_confidence \u001b[39min\u001b[39;00m sample_confidences]\n\u001b[1;32m    107\u001b[0m \u001b[39m# We're ok with up to 1 match, but every one more than that is a conflict.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [3], line 100\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     95\u001b[0m state_space_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mmultivariate_normal(sample_pdf\u001b[39m.\u001b[39mmean, np\u001b[39m.\u001b[39mdiag(sample_pdf\u001b[39m.\u001b[39mstd), samples)\n\u001b[1;32m     97\u001b[0m \u001b[39m# using probability_of_multivariant_point no longer directly equates to false negative rate.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m# TODO (henry): Figure out relationship between integrated pdf range and false negative rate\u001b[39;00m\n\u001b[1;32m     99\u001b[0m sample_confidences \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 100\u001b[0m     [probability_of_multivariant_point(pdf\u001b[39m.\u001b[39mmean, np\u001b[39m.\u001b[39mdiag(pdf\u001b[39m.\u001b[39mstd), sample) \u001b[39mfor\u001b[39;00m pdf \u001b[39min\u001b[39;00m part_pdfs]\n\u001b[1;32m    101\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m state_space_samples]\n\u001b[1;32m    103\u001b[0m filtered_confidences \u001b[39m=\u001b[39m [\n\u001b[1;32m    104\u001b[0m     \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m confidence: confidence \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_confidence, sample_confidence))\n\u001b[1;32m    105\u001b[0m     \u001b[39mfor\u001b[39;00m sample_confidence \u001b[39min\u001b[39;00m sample_confidences]\n\u001b[1;32m    107\u001b[0m \u001b[39m# We're ok with up to 1 match, but every one more than that is a conflict.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [3], line 100\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     95\u001b[0m state_space_samples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mmultivariate_normal(sample_pdf\u001b[39m.\u001b[39mmean, np\u001b[39m.\u001b[39mdiag(sample_pdf\u001b[39m.\u001b[39mstd), samples)\n\u001b[1;32m     97\u001b[0m \u001b[39m# using probability_of_multivariant_point no longer directly equates to false negative rate.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m# TODO (henry): Figure out relationship between integrated pdf range and false negative rate\u001b[39;00m\n\u001b[1;32m     99\u001b[0m sample_confidences \u001b[39m=\u001b[39m [\n\u001b[0;32m--> 100\u001b[0m     [probability_of_multivariant_point(pdf\u001b[39m.\u001b[39;49mmean, np\u001b[39m.\u001b[39;49mdiag(pdf\u001b[39m.\u001b[39;49mstd), sample) \u001b[39mfor\u001b[39;00m pdf \u001b[39min\u001b[39;00m part_pdfs]\n\u001b[1;32m    101\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m state_space_samples]\n\u001b[1;32m    103\u001b[0m filtered_confidences \u001b[39m=\u001b[39m [\n\u001b[1;32m    104\u001b[0m     \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m confidence: confidence \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_confidence, sample_confidence))\n\u001b[1;32m    105\u001b[0m     \u001b[39mfor\u001b[39;00m sample_confidence \u001b[39min\u001b[39;00m sample_confidences]\n\u001b[1;32m    107\u001b[0m \u001b[39m# We're ok with up to 1 match, but every one more than that is a conflict.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [3], line 78\u001b[0m, in \u001b[0;36mprobability_of_multivariant_point\u001b[0;34m(mu, cov, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprobability_of_multivariant_point\u001b[39m(mu: List[\u001b[39mfloat\u001b[39m], cov: List[List[\u001b[39mfloat\u001b[39m]], x: List[\u001b[39mfloat\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[1;32m     75\u001b[0m \n\u001b[1;32m     76\u001b[0m     \u001b[39m#https://stats.stackexchange.com/questions/331283/how-to-calculate-the-probability-of-a-data-point-belonging-to-a-multivariate-nor\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39m# Double check this math\u001b[39;00m\n\u001b[0;32m---> 78\u001b[0m     m_dist_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot((x\u001b[39m-\u001b[39mmu)\u001b[39m.\u001b[39mtranspose(),np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(cov))\n\u001b[1;32m     79\u001b[0m     m_dist_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(m_dist_x, (x\u001b[39m-\u001b[39mmu))\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m\u001b[39m-\u001b[39mstats\u001b[39m.\u001b[39mchi2\u001b[39m.\u001b[39mcdf(m_dist_x, \u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/cs-6362/lib/python3.10/site-packages/numpy/linalg/linalg.py:552\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    550\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39md->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    551\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 552\u001b[0m ainv \u001b[39m=\u001b[39m _umath_linalg\u001b[39m.\u001b[39;49minv(a, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[1;32m    553\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(ainv\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def run_experiment(part_type: str, part_dim: int, num_samples: int, meta_pdf_ci: float, part_pdf_ci: float, confidence_bound: float):\n",
    "\n",
    "    con_parts = load_part_data(part_type)\n",
    "    estimated_upper_collision_rate = run_meta_markov_multivariant_analysis(\n",
    "    con_parts, part_dim, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\n",
    "    print(f\"Upper collision rate: {estimated_upper_collision_rate * 100}%\")\n",
    "\n",
    "\n",
    "            \n",
    "# experiment = mlflow.set_experiment(\"Experiment 2\")\n",
    "\n",
    "# part_types = [\"BEAM\", \"BOX\", \"BRK\", \"CON\", \"CONLID\", \"FLG\", \"IMP\", \"LID\", \"SEN\", \"TUBE\", \"VNT\"]\n",
    "# part_dims = [1, 5, 10, 50, 100, 500]\n",
    "# base_num_samples = 100 # We can keep this constant; it doesn't really need to be a hyperparameter\n",
    "# meta_pdf_cis = [0.5, 0.8, 0.9, 0.95, 0.99, 0.999]\n",
    "# part_pdf_cis = [0.5, 0.8, 0.9, 0.95, 0.99, 0.999]\n",
    "# confidence_bounds = [0.5, 0.8, 0.9, 0.95, 0.99, 0.999]\n",
    "\n",
    "    \n",
    "# for base_part_type in part_types:\n",
    "#     for base_part_dim in part_dims:\n",
    "#         for base_meta_pdf_ci in meta_pdf_cis:\n",
    "#             for base_part_pdf_ci in part_pdf_cis:\n",
    "#                 for base_confidence_bound in confidence_bounds:\n",
    "#                     with mlflow.start_run():\n",
    "\n",
    "#                         mlflow.log_param(\"part_type\", base_part_type)\n",
    "#                         mlflow.log_param(\"part_dim\", base_part_dim)\n",
    "#                         mlflow.log_param(\"num_samples\", base_num_samples)\n",
    "#                         mlflow.log_param(\"meta_pdf_ci\", base_meta_pdf_ci)\n",
    "#                         mlflow.log_param(\"part_pdf_ci\", base_part_pdf_ci)\n",
    "#                         mlflow.log_param(\"confidence_bound\", base_confidence_bound)\n",
    "\n",
    "#                         run_experiment(base_part_type, base_part_dim, base_num_samples, base_meta_pdf_ci, base_part_pdf_ci, base_confidence_bound)\n",
    "\n",
    "#                         mlflow.end_run()\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_values = {\n",
    "    'part_type': [\"CON\"],\n",
    "    'part_dim' : [1, 5, 10, 50],\n",
    "    'num_samples': [100],\n",
    "    'meta_pdf_ci' : [0.5, 0.8, 0.9, 0.95, 0.99, 0.999],\n",
    "    'part_pdf_ci' : [0.5, 0.8, 0.9, 0.95, 0.99, 0.999],\n",
    "    'confidence_bound' : [0.5, 0.8, 0.9, 0.95, 0.99, 0.999]}\n",
    "\n",
    "parameter_grid = list(ParameterGrid(param_values))\n",
    "mlflow.set_experiment(\"Experiment 2\")\n",
    "\n",
    "for params in parameter_grid:\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(params)\n",
    "        run_experiment(**params)\n",
    "        mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cs-6362')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7dd7eb3fce35fa0f50760c8f8b3d129dbc0da5e6df1057aa9ef5bcae08959f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
