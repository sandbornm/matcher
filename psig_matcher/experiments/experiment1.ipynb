{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Part Signal PDF Convergence\n",
    "\n",
    "This notebook will contain gathered results from experiment 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Signal PDF convergence\n",
    "## Methodology \n",
    "For each part we have data for, run the function to simulate the PDF convergence, 100 times. Randomly shuffle the part signals between each run, otherwise it would yeild the same results each time. Track the part, part type, how many signals it needed until convergence, and the relative variance. \n",
    "## Deliverables\n",
    "Associated graphs for each part run showing the the convergence of the CI\n",
    "Graphs and analysis for the combined average of each part type. What does it tell us? What can we conclude about the part type and why it is behaving that way?\n",
    "Graphs and analysis comparing the averages of the different types. How different are they? How can we explain this? Does this validate our assumptions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Environment\n",
    "Installing required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing Directory to Parent to allow importing external data files. Please change the specified path based on where this repo exists for you locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "user_path = '~/matcher'  # CHANGE THIS LINE AS NEEDED FOR YOUR ENVIRONMENT\n",
    "os.chdir(os.path.expanduser(user_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code\n",
    "\n",
    "The below sections contains all of our source codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import scipy.stats as st\n",
    "from scipy import stats\n",
    "import dataclasses\n",
    "import argparse\n",
    "import sys\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.artifacts import download_artifacts\n",
    "\n",
    "@dataclass\n",
    "class NormalDistribution:\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "@dataclass\n",
    "class Part:\n",
    "\n",
    "    type: str\n",
    "    sub_part_name: str\n",
    "    sensor: str\n",
    "    signals: List   # Signal is numpy array of (500,3) with [frequency, Z, X]\n",
    "\n",
    "\n",
    "def load_part_data(part_type: str) -> List[Part]:\n",
    "\n",
    "    parts = []\n",
    "    for part_dir in os.listdir(f'psig_matcher/data/{part_type}'):\n",
    "\n",
    "        sensor = part_dir[1:]\n",
    "        measurement_files = glob.glob(f'psig_matcher/data/{part_type}/{part_dir}/*.npy')\n",
    "        measurements = [np.load(f) for f in measurement_files]\n",
    "        parts.append(Part(part_type, part_dir, sensor, measurements))\n",
    "\n",
    "    return parts\n",
    "\n",
    "def limit_deminsionality(parts: List[Part], frequeny_indexes: List[int]) -> List[Part]:\n",
    "    \"\"\"Use only a subset of the frequencies for the analysis. This effectivley transforms the\n",
    "    500 dimension multivariant distribution to a n-dimentional distribution where n is the\n",
    "    length of the frequency_indexes.\n",
    "    Further, this assumes use of the X axis\"\"\"\n",
    "\n",
    "    return [\n",
    "        dataclasses.replace(part, signals=[[signal[index][1] for index in frequeny_indexes] for signal in part.signals])\n",
    "        for part in parts]\n",
    "\n",
    "def compute_normal_ci(x: List[float], confidence: float) -> Tuple[float, float]:\n",
    "    \"\"\"Computes the confidence interval for a given confidence bound.\"\"\"\n",
    "\n",
    "    if sum(x) == 0: return (0, 0)\n",
    "\n",
    "    if len(x) < 30:\n",
    "        return st.t.interval(confidence, len(x)-1, loc=np.mean(x), scale=st.sem(x))\n",
    "    else:\n",
    "        return stats.norm.interval(confidence, loc=np.mean(x), scale=np.std(x))\n",
    "\n",
    "def estimate_normal_dist(x: List[float], confidence: float) -> NormalDistribution:\n",
    "    \"\"\"Estimate the normal distribution for the given data.\n",
    "    This is done using: https://handbook-5-1.cochrane.org/chapter_7/7_7_3_2_obtaining_standard_deviations_from_standard_errors_and.htm#:~:text=The%20standard%20deviation%20for%20each,should%20be%20replaced%20by%205.15.\n",
    "    \"\"\"\n",
    "\n",
    "    val_comp = st.t.ppf if len(x) < 30 else stats.norm.ppf\n",
    "    lower, upper = compute_normal_ci(x, confidence)\n",
    "\n",
    "    val = val_comp(confidence, len(x)-1)\n",
    "    std = np.sqrt(len(x))*(upper-lower)*val\n",
    "    return NormalDistribution(np.mean(x, axis=0), std)\n",
    "\n",
    "\n",
    "def probability_of_multivariant_point(mu: List[float], cov: List[List[float]], x: List[float]) -> float:\n",
    "\n",
    "    #https://stats.stackexchange.com/questions/331283/how-to-calculate-the-probability-of-a-data-point-belonging-to-a-multivariate-nor\n",
    "    # Double check this math\n",
    "    m_dist_x = np.dot((x-mu).transpose(),np.linalg.inv(cov))\n",
    "    m_dist_x = np.dot(m_dist_x, (x-mu))\n",
    "    return 1-stats.chi2.cdf(m_dist_x, 3)\n",
    "\n",
    "def estimate_overlap_of_set_with_sample_signals(parts: List[Part], samples: int, meta_pdf_ci: float, part_pdf_ci: float, confidence_bound: float) -> float:\n",
    "    \"\"\" I believe this is the best solution out of all them. We are directly modeling the distribution/state space that\n",
    "    the signals come from, and sampling from that. This directly correlates with the CI and is intuitive. See notion for\n",
    "    more details and defense. \"\"\"\n",
    "\n",
    "    min_confidence = 1 - confidence_bound\n",
    "    signals = [\n",
    "        signal for part in parts\n",
    "        for signal in part.signals]\n",
    "\n",
    "    part_pdfs = [estimate_normal_dist(part.signals, part_pdf_ci) for part in parts]\n",
    "    sample_pdf = estimate_normal_dist(signals, meta_pdf_ci)\n",
    "\n",
    "    state_space_samples = np.random.multivariate_normal(sample_pdf.mean, np.diag(sample_pdf.std), samples)\n",
    "\n",
    "    # using probability_of_multivariant_point no longer directly equates to false negative rate.\n",
    "    # TODO (henry): Figure out relationship between integrated pdf range and false negative rate\n",
    "    sample_confidences = [\n",
    "        [probability_of_multivariant_point(pdf.mean, np.diag(pdf.std), sample) for pdf in part_pdfs]\n",
    "        for sample in state_space_samples]\n",
    "\n",
    "    filtered_confidences = [\n",
    "        list(filter(lambda confidence: confidence >= min_confidence, sample_confidence))\n",
    "        for sample_confidence in sample_confidences]\n",
    "\n",
    "    # We're ok with up to 1 match, but every one more than that is a conflict.\n",
    "    collisions = [max(len(confidences)-1, 0) for confidences in filtered_confidences]\n",
    "    return sum(collisions)/(samples*len(part_pdfs))\n",
    "\n",
    "def run_meta_markov_multivariant_analysis(parts: List[Part], part_dim: int, num_samples: int, meta_pdf_ci: float, part_pdf_ci: float, confidence_bound: float):\n",
    "    \"\"\" Runs the Monte Carlo Approximation of multivariant collision using the signal sample meta\n",
    "    pdf methodology. The Monte Carlo Approximation will continually be run until the confidence interval\n",
    "    converges and the average of the previous 10 runs is not smaller than the average of the previous 100 runs.\"\"\"\n",
    "\n",
    "    collisions = []\n",
    "    confidence_ranges = []\n",
    "    while True:\n",
    "\n",
    "        multivariant_parts = limit_deminsionality(parts, list(range(part_dim)))\n",
    "        collision_rate = estimate_overlap_of_set_with_sample_signals(multivariant_parts, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\n",
    "\n",
    "        collisions.append(collision_rate)\n",
    "        mlflow.log_metric(\"monte_carlo_collision_rate\", collision_rate)\n",
    "\n",
    "        lower, upper = compute_normal_ci(collisions, 0.95)\n",
    "        confidence_ranges.append(upper - lower)\n",
    "        mlflow.log_metric(\"monte_carlo_confidence_interval\", upper - lower)\n",
    "\n",
    "        # print(f\"Estimated collision rate from sample distributiion has range: {upper - lower}\")\n",
    "\n",
    "        if len(confidence_ranges) > 100 and np.mean(confidence_ranges[-10:]) >= np.mean(confidence_ranges[-100:]):\n",
    "            return upper\n",
    "\n",
    "def simulate_part_pdf_convergence(part_signals: np.ndarray, part_dim: int, part_pdf_ci: float):\n",
    "    \"\"\" Given a discrete set of signals, this will simulate the part PDF CI convergence methodology.\n",
    "    This function pretend that the set of the signals is infinite, Also incorporate logic to handle\n",
    "    if we didn't converge before we ran out of data. Have modular connection style such that we could\n",
    "    add streaming data source in the future. \"\"\"\n",
    "    sub_samples = []\n",
    "    confidence_ranges = []\n",
    "    while part_signals:\n",
    "        # The below segment is equivalent to sub_samples.append(part_signals.pop())\n",
    "        part_sig = part_signals[0]\n",
    "        part_signals = part_signals[1:]\n",
    "        sub_samples.append(part_sig)\n",
    "\n",
    "        lower, upper = compute_normal_ci(sub_samples, part_pdf_ci)\n",
    "        confidence_ranges.append(upper - lower)\n",
    "\n",
    "        mlflow.log_metric(\"part_pdf_confidence_interval\", upper - lower)\n",
    "\n",
    "        if len(confidence_range) > 100 and np.mean(confidence_ranges[-10:]) >= np.mean(confidence_ranges[-100:]):\n",
    "            return upper\n",
    "            \n",
    "def run_experiment(part_type: str, part_dim: int, num_samples: int, meta_pdf_ci: float, part_pdf_ci: float, confidence_bound: float):\n",
    "\n",
    "    con_parts = load_part_data(part_type)\n",
    "    upper_collision_rate = run_meta_markov_multivariant_analysis(con_parts, part_dim, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\n",
    "    print(f\"Upper collision rate: {upper_collision_rate * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "The below sections gives example scenarios to illustrate the working code and validate the proposed approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Base Line*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m part_pdf_ci\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.999\u001b[39m\n\u001b[0;32m      8\u001b[0m confidence_bound\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.999\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_pdf_ci\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_pdf_ci\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_bound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, part_type)\n\u001b[0;32m     14\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mlog_param(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m, part_dim)\n",
      "Cell \u001b[1;32mIn[9], line 159\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(part_type, part_dim, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_experiment\u001b[39m(part_type: \u001b[38;5;28mstr\u001b[39m, part_dim: \u001b[38;5;28mint\u001b[39m, num_samples: \u001b[38;5;28mint\u001b[39m, meta_pdf_ci: \u001b[38;5;28mfloat\u001b[39m, part_pdf_ci: \u001b[38;5;28mfloat\u001b[39m, confidence_bound: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m    158\u001b[0m     con_parts \u001b[38;5;241m=\u001b[39m load_part_data(part_type)\n\u001b[1;32m--> 159\u001b[0m     upper_collision_rate \u001b[38;5;241m=\u001b[39m \u001b[43mrun_meta_markov_multivariant_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcon_parts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_pdf_ci\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_pdf_ci\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_bound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper collision rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mupper_collision_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 121\u001b[0m, in \u001b[0;36mrun_meta_markov_multivariant_analysis\u001b[1;34m(parts, part_dim, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    120\u001b[0m     multivariant_parts \u001b[38;5;241m=\u001b[39m limit_deminsionality(parts, \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(part_dim)))\n\u001b[1;32m--> 121\u001b[0m     collision_rate \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_overlap_of_set_with_sample_signals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmultivariant_parts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta_pdf_ci\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_pdf_ci\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence_bound\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     collisions\u001b[38;5;241m.\u001b[39mappend(collision_rate)\n\u001b[0;32m    124\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonte_carlo_collision_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, collision_rate)\n",
      "Cell \u001b[1;32mIn[9], line 92\u001b[0m, in \u001b[0;36mestimate_overlap_of_set_with_sample_signals\u001b[1;34m(parts, samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\u001b[0m\n\u001b[0;32m     87\u001b[0m min_confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m confidence_bound\n\u001b[0;32m     88\u001b[0m signals \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     89\u001b[0m     signal \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m signal \u001b[38;5;129;01min\u001b[39;00m part\u001b[38;5;241m.\u001b[39msignals]\n\u001b[1;32m---> 92\u001b[0m part_pdfs \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mestimate_normal_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_pdf_ci\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparts\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     93\u001b[0m sample_pdf \u001b[38;5;241m=\u001b[39m estimate_normal_dist(signals, meta_pdf_ci)\n\u001b[0;32m     95\u001b[0m state_space_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmultivariate_normal(sample_pdf\u001b[38;5;241m.\u001b[39mmean, np\u001b[38;5;241m.\u001b[39mdiag(sample_pdf\u001b[38;5;241m.\u001b[39mstd), samples)\n",
      "Cell \u001b[1;32mIn[9], line 92\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     87\u001b[0m min_confidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m confidence_bound\n\u001b[0;32m     88\u001b[0m signals \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     89\u001b[0m     signal \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m signal \u001b[38;5;129;01min\u001b[39;00m part\u001b[38;5;241m.\u001b[39msignals]\n\u001b[1;32m---> 92\u001b[0m part_pdfs \u001b[38;5;241m=\u001b[39m [\u001b[43mestimate_normal_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpart_pdf_ci\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m parts]\n\u001b[0;32m     93\u001b[0m sample_pdf \u001b[38;5;241m=\u001b[39m estimate_normal_dist(signals, meta_pdf_ci)\n\u001b[0;32m     95\u001b[0m state_space_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mmultivariate_normal(sample_pdf\u001b[38;5;241m.\u001b[39mmean, np\u001b[38;5;241m.\u001b[39mdiag(sample_pdf\u001b[38;5;241m.\u001b[39mstd), samples)\n",
      "Cell \u001b[1;32mIn[9], line 67\u001b[0m, in \u001b[0;36mestimate_normal_dist\u001b[1;34m(x, confidence)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"Estimate the normal distribution for the given data.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03mThis is done using: https://handbook-5-1.cochrane.org/chapter_7/7_7_3_2_obtaining_standard_deviations_from_standard_errors_and.htm#:~:text=The%20standard%20deviation%20for%20each,should%20be%20replaced%20by%205.15.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m val_comp \u001b[38;5;241m=\u001b[39m st\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m.\u001b[39mppf \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m stats\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mppf\n\u001b[1;32m---> 67\u001b[0m lower, upper \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_normal_ci\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfidence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m val \u001b[38;5;241m=\u001b[39m val_comp(confidence, \u001b[38;5;28mlen\u001b[39m(x)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     70\u001b[0m std \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mlen\u001b[39m(x))\u001b[38;5;241m*\u001b[39m(upper\u001b[38;5;241m-\u001b[39mlower)\u001b[38;5;241m*\u001b[39mval\n",
      "Cell \u001b[1;32mIn[9], line 54\u001b[0m, in \u001b[0;36mcompute_normal_ci\u001b[1;34m(x, confidence)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_normal_ci\u001b[39m(x: List[\u001b[38;5;28mfloat\u001b[39m], confidence: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;124;03m\"\"\"Computes the confidence interval for a given confidence bound.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m30\u001b[39m:\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m st\u001b[38;5;241m.\u001b[39mt\u001b[38;5;241m.\u001b[39minterval(confidence, \u001b[38;5;28mlen\u001b[39m(x)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, loc\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmean(x), scale\u001b[38;5;241m=\u001b[39mst\u001b[38;5;241m.\u001b[39msem(x))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "# Let's first establish a base line result using the following values\n",
    "\n",
    "part_type=\"CON\"\n",
    "part_dim=5\n",
    "num_samples=100\n",
    "meta_pdf_ci=0.999\n",
    "part_pdf_ci=0.999\n",
    "confidence_bound=0.999\n",
    "\n",
    "run_experiment(part_type, part_dim, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\n",
    "\n",
    "\n",
    "mlflow.log_param(\"part_type\", part_type)\n",
    "mlflow.log_param(\"part_dim\", part_dim)\n",
    "mlflow.log_param(\"num_samples\", num_samples)\n",
    "mlflow.log_param(\"meta_pdf_ci\", meta_pdf_ci)\n",
    "mlflow.log_param(\"part_pdf_ci\", part_pdf_ci)\n",
    "mlflow.log_param(\"confidence_bound\", confidence_bound)\n",
    "\n",
    "con_parts = load_part_data(part_type)\n",
    "estimated_upper_collision_rate = run_meta_markov_multivariant_analysis(\n",
    "    con_parts, part_dim, num_samples, meta_pdf_ci, part_pdf_ci, confidence_bound)\n",
    "print(f\"Upper collision rate: {estimated_upper_collision_rate * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e408d6d2624d2574650b7f4ce724a272157838fb62dd59ce9f909f9eb3ba3f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
