{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Part Signal PDF Convergence\n",
    "\n",
    "This notebook will contain gathered results from experiment 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Signal PDF convergence\n",
    "## Methodology \n",
    "For each part we have data for, run the function to simulate the PDF convergence, 100 times. Randomly shuffle the part signals between each run, otherwise it would yeild the same results each time. Track the part, part type, how many signals it needed until convergence, and the relative variance. \n",
    "## Deliverables\n",
    "Associated graphs for each part run showing the the convergence of the CI\n",
    "Graphs and analysis for the combined average of each part type. What does it tell us? What can we conclude about the part type and why it is behaving that way?\n",
    "Graphs and analysis comparing the averages of the different types. How different are they? How can we explain this? Does this validate our assumptions? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Environment\n",
    "Installing required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing Directory to Parent to allow importing external data files. Please change the specified path based on where this repo exists for you locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "user_path = '~/matcher'  # CHANGE THIS LINE AS NEEDED FOR YOUR ENVIRONMENT\n",
    "os.chdir(os.path.expanduser(user_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Code\n",
    "\n",
    "The below sections contains all of our source codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import scipy.stats as st\n",
    "from scipy import stats\n",
    "import dataclasses\n",
    "import argparse\n",
    "import sys\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.artifacts import download_artifacts\n",
    "\n",
    "@dataclass\n",
    "class NormalDistribution:\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "@dataclass\n",
    "class Part:\n",
    "\n",
    "    type: str\n",
    "    sub_part_name: str\n",
    "    sensor: str\n",
    "    signals: List   # Signal is numpy array of (500,3) with [frequency, Z, X]\n",
    "\n",
    "\n",
    "def load_part_data(part_type: str) -> List[Part]:\n",
    "\n",
    "    parts = []\n",
    "    for part_dir in os.listdir(f'psig_matcher/data/{part_type}'):\n",
    "\n",
    "        sensor = part_dir[1:]\n",
    "        measurement_files = glob.glob(f'psig_matcher/data/{part_type}/{part_dir}/*.npy')\n",
    "        measurements = [np.load(f) for f in measurement_files]\n",
    "        parts.append(Part(part_type, part_dir, sensor, measurements))\n",
    "\n",
    "    return parts\n",
    "\n",
    "def limit_deminsionality(parts: List[Part], frequeny_indexes: List[int]) -> List[Part]:\n",
    "    \"\"\"Use only a subset of the frequencies for the analysis. This effectivley transforms the\n",
    "    500 dimension multivariant distribution to a n-dimentional distribution where n is the\n",
    "    length of the frequency_indexes.\n",
    "    Further, this assumes use of the X axis\"\"\"\n",
    "\n",
    "    return [\n",
    "        dataclasses.replace(part, signals=[[signal[index][1] for index in frequeny_indexes] for signal in part.signals])\n",
    "        for part in parts]\n",
    "\n",
    "def compute_normal_ci(x: List[float], confidence: float) -> Tuple[float, float]:\n",
    "    \"\"\"Computes the confidence interval for a given confidence bound.\"\"\"\n",
    "    if np.mean(x) == 0: return (0, 0)\n",
    "    \n",
    "    if len(x) < 30:\n",
    "        return st.t.interval(confidence, len(x)-1, loc=np.mean(x), scale=st.sem(x))\n",
    "    else:\n",
    "        return stats.norm.interval(confidence, loc=np.mean(x), scale=np.std(x))\n",
    "\n",
    "def estimate_normal_dist(x: List[float], confidence: float) -> NormalDistribution:\n",
    "    \"\"\"Estimate the normal distribution for the given data.\n",
    "    This is done using: https://handbook-5-1.cochrane.org/chapter_7/7_7_3_2_obtaining_standard_deviations_from_standard_errors_and.htm#:~:text=The%20standard%20deviation%20for%20each,should%20be%20replaced%20by%205.15.\n",
    "    \"\"\"\n",
    "\n",
    "    val_comp = st.t.ppf if len(x) < 30 else stats.norm.ppf\n",
    "    lower, upper = compute_normal_ci(x, confidence)\n",
    "\n",
    "    val = val_comp(confidence, len(x)-1)\n",
    "    std = np.sqrt(len(x))*(upper-lower)*val\n",
    "    return NormalDistribution(np.mean(x, axis=0), std)\n",
    "\n",
    "\n",
    "def probability_of_multivariant_point(mu: List[float], cov: List[List[float]], x: List[float]) -> float:\n",
    "\n",
    "    #https://stats.stackexchange.com/questions/331283/how-to-calculate-the-probability-of-a-data-point-belonging-to-a-multivariate-nor\n",
    "    # Double check this math\n",
    "    m_dist_x = np.dot((x-mu).transpose(),np.linalg.inv(cov))\n",
    "    m_dist_x = np.dot(m_dist_x, (x-mu))\n",
    "    return 1-stats.chi2.cdf(m_dist_x, 3)\n",
    "\n",
    "def estimate_overlap_of_set_with_sample_signals(parts: List[Part], samples: int, meta_pdf_ci: float, part_pdf_ci: float, confidence_bound: float) -> float:\n",
    "    \"\"\" I believe this is the best solution out of all them. We are directly modeling the distribution/state space that\n",
    "    the signals come from, and sampling from that. This directly correlates with the CI and is intuitive. See notion for\n",
    "    more details and defense. \"\"\"\n",
    "\n",
    "    min_confidence = 1 - confidence_bound\n",
    "    signals = [\n",
    "        signal for part in parts\n",
    "        for signal in part.signals]\n",
    "\n",
    "    part_pdfs = [estimate_normal_dist(part.signals, part_pdf_ci) for part in parts]\n",
    "    sample_pdf = estimate_normal_dist(signals, meta_pdf_ci)\n",
    "    \n",
    "    state_space_samples = np.random.multivariate_normal(sample_pdf.mean, np.diag(sample_pdf.std), samples)\n",
    "\n",
    "    # using probability_of_multivariant_point no longer directly equates to false negative rate.\n",
    "    # TODO (henry): Figure out relationship between integrated pdf range and false negative rate\n",
    "    sample_confidences = [\n",
    "        [probability_of_multivariant_point(pdf.mean, np.diag(pdf.std), sample) for pdf in part_pdfs]\n",
    "        for sample in state_space_samples]\n",
    "\n",
    "    filtered_confidences = [\n",
    "        list(filter(lambda confidence: confidence >= min_confidence, sample_confidence))\n",
    "        for sample_confidence in sample_confidences]\n",
    "\n",
    "    # We're ok with up to 1 match, but every one more than that is a conflict.\n",
    "    collisions = [max(len(confidences)-1, 0) for confidences in filtered_confidences]\n",
    "    return sum(collisions)/(samples*len(part_pdfs))\n",
    "\n",
    "def simulate_part_pdf_convergence(part_signals: np.ndarray, part_dim: int, part_pdf_ci: float):\n",
    "    \"\"\" Given a discrete set of signals, this will simulate the part PDF CI convergence methodology.\n",
    "    This function pretend that the set of the signals is infinite, Also incorporate logic to handle\n",
    "    if we didn't converge before we ran out of data. Have modular connection style such that we could\n",
    "    add streaming data source in the future. \"\"\"\n",
    "    sub_samples = []\n",
    "    confidence_ranges = []\n",
    "    while len(part_signals) != 0:\n",
    "        # The below segment is equivalent to sub_samples.append(part_signals.pop())\n",
    "        part_sig = part_signals[0:5]\n",
    "        part_signals = part_signals[6:]\n",
    "        sub_samples += part_sig\n",
    "        \n",
    "        lower, upper = compute_normal_ci(sub_samples, part_pdf_ci)\n",
    "        confidence_ranges.append(upper - lower)\n",
    "        print(\"lower\", lower)\n",
    "        print(\"upper\", upper)\n",
    "        \n",
    "        mlflow.log_metric(\"part_pdf_confidence_interval\", np.mean(upper - lower))\n",
    "\n",
    "        if len(confidence_ranges) > 100 and np.mean(confidence_ranges[-10:]) >= np.mean(confidence_ranges[-100:]):\n",
    "            return upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "\n",
    "The below sections gives example scenarios to illustrate the working code and validate the proposed approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/12/01 18:22:45 INFO mlflow.tracking.fluent: Experiment with name 'Experiment 1' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ax1\n",
      "lower [1156.78042281 1151.53330824 1145.67369673 1142.8874194  1157.21707992]\n",
      "upper [1186.61221719 1191.85933176 1197.71894327 1200.5052206  1186.17556008]\n",
      "final upper collision rate - None\n",
      "Bx2\n",
      "lower [1194.08840522 1185.90808932 1176.44790599 1185.45623055 1189.23988045]\n",
      "upper [1216.93863478 1225.11895068 1234.57913401 1225.57080945 1221.78715955]\n",
      "lower [1171.46650302 1183.81866371 1163.37438281 1129.87112461 1103.62769759]\n",
      "upper [1209.05936364 1196.70720295 1217.15148386 1250.65474205 1276.89816908]\n",
      "final upper collision rate - None\n",
      "Cx3\n",
      "lower [1305.61474975 1325.04043266 1326.82997549 1322.19330604 1335.04531576]\n",
      "upper [1390.22757025 1370.80188734 1369.01234451 1373.64901396 1360.79700424]\n",
      "final upper collision rate - None\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(part_type: str, part_dim: int, num_samples: int, meta_pdf_ci: float, part_pdf_ci: float, confidence_bound: float):\n",
    "    con_parts = load_part_data(part_type)\n",
    "    con_parts = limit_deminsionality(con_parts, list(range(part_dim)))\n",
    "    for con_part in con_parts:\n",
    "        print(con_part.sub_part_name)\n",
    "        upper_collision_rate = simulate_part_pdf_convergence(con_part.signals, part_dim, part_pdf_ci)\n",
    "        print(\"final upper collision rate -\", upper_collision_rate)\n",
    "\n",
    "experiment = mlflow.set_experiment(\"Experiment 1\")\n",
    "with mlflow.start_run():    \n",
    "    base_part_type=\"CON\"\n",
    "    base_part_dim=2\n",
    "    base_num_samples=100\n",
    "    base_meta_pdf_ci=0.999\n",
    "    base_part_pdf_ci=0.999\n",
    "    base_confidence_bound=0.999\n",
    "\n",
    "    mlflow.log_param(\"part_type\", base_part_type)\n",
    "    mlflow.log_param(\"part_dim\", base_part_dim)\n",
    "    mlflow.log_param(\"num_samples\", base_num_samples)\n",
    "    mlflow.log_param(\"meta_pdf_ci\", base_meta_pdf_ci)\n",
    "    mlflow.log_param(\"part_pdf_ci\", base_part_pdf_ci)\n",
    "    mlflow.log_param(\"confidence_bound\", base_confidence_bound)\n",
    "\n",
    "    run_experiment(base_part_type, base_part_dim, base_num_samples, base_meta_pdf_ci, base_part_pdf_ci, base_confidence_bound)\n",
    "    \n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e408d6d2624d2574650b7f4ce724a272157838fb62dd59ce9f909f9eb3ba3f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
